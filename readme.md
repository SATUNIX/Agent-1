# Agent Network System (Build 1) (Experimental - Untested)

An **offlineâ€‘first agent network** that combines two complementary pipelines:

| Pipeline    | Entryâ€‘point                   | Purpose                                                                                                                                                     |
| ----------- | ----------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Classic** | `python run.py "<goal>"`      | PlannerÂ â†’Â WriterÂ â†’Â Openâ€‘Interpreter DevAgent. Great for documentationâ€‘heavy tasks that also need live code edits with sandboxed execution.                  |
| **Modern**  | `python run_loop.py "<goal>"` | ManagerAgentÂ â†’Â LLMDevAgent. Demonstrates sharedâ€memory coordination and tokenâ€‘aware trimming when you want pure LLM reasoning with optional code execution. |

> **One machine â€“ one inference at a time.**  Both pipelines enforce sequential LLM calls so a single Ollama instance (CPU or GPU) is never doubleâ€‘booked.

---

## 1Â Â System Diagram

```mermaid
graph TD
    subgraph Classic
        U(User)-- goal -->P(PlannerAgent)
        P-->W(WriterAgent)
        P-->D1(Openâ€‘Interpreter DevAgent)
        W-->Docs
        D1-->Git
    end

    subgraph Modern
        U-.->M(ManagerAgent)
        M-->D2(LLMDevAgent)
        D2-->M
        D2-->Git2
    end

    Memory((SharedÂ Memory))
    P-- read/write -->Memory
    W-- read/write -->Memory
    M-- read/write -->Memory
    D2-- read/write -->Memory
```

---

## 2Â Â Hardware Requirements

| Component | Minimum                    | Recommended                                                                   |
| --------- | -------------------------- | ----------------------------------------------------------------------------- |
| **CPU**   | Any 4â€‘core x64             | 6+ cores for parallel build/test during code execution                        |
| **RAM**   | 8Â GB                       | 16Â GB+ (DeepSeekâ€‘R1Â 7B quantised uses \~6â€¯GB)                                 |
| **GPU**   | *(Optional)* Integrated    | NVIDIAÂ RTXÂ 30â€‘series or IntelÂ ARC with â‰¥8â€¯GB vRAM for faster Ollama inference |
| **Disk**  | 5Â GB free (models)         | 15Â GB+ if you pull multiple models (DeepSeekâ€‘Coder, CodeLlama, etc.)          |
| **OS**    | WindowsÂ 10Â 64â€‘bit or later | WindowsÂ 11Â 22H2 with WSLÂ 2 for Linuxâ€‘style tooling                            |

> *The system is equally happy on Linux/macOS, but the scripts assume a Windows host path for temporary files.*

---

## 3Â Â Software Stack

* **PythonÂ â‰¥Â 3.10** â€ƒ(`python --version`)
* **OllamaÂ â‰¥Â 0.1.23**â€ƒ(local LLM server)
* **GitÂ for Windows**â€ƒ( commandâ€‘line `git` )
* **VisualÂ StudioÂ Build Tools** *(only if Openâ€‘Interpreter needs C++ extensions)*
* **PythonÂ packages** â€“ install via `pip install -r requirements.txt`

  * `openâ€‘interpreter`, `duckduckgoâ€‘search`, `beautifulsoup4`, `lxml`, `requests`, `pytest`, *optional* `tiktoken`, `ollamaâ€‘python`.

### 3.1Â Â Pulling Models

```powershell
# DeepSeekâ€‘R1 7B for reasoning / planning
ollama pull deepseek-r1:7b
# DeepSeekâ€‘Coder for codeâ€‘centric DevAgent
ollama pull deepseek-coder
```

> Quantised (`q4_K_M`) models work fine; they reduce VRAM/RAM usage \~40%.

---

## 4Â Â Setup

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt

# optional envâ€‘vars ( PowerShell )
$env:OLLAMA_URL = "http://localhost:11434"
$env:DEV_MODEL   = "deepseek-coder"
$env:AGENT_MODEL = "deepseek-r1:7b"
```
Create an environment file (not provided in this repository) 
Template below. 

```
OLLAMA_API=http://localhost:11434/api/generate
# Models
PLANNER_MODEL=deepseek-r1:7b
WRITER_MODEL=deepseek-r1:7b
DEV_MODEL=deepseek-coder
TEST_COMMAND=pytest         

```

Create a fresh Git repo so snapshots work:

```powershell
git init
```

---

## 5Â Â Usage

### 5.1Â Â Documentationâ€‘first (Classic) Pipeline

```powershell
python run.py "Document and implement a CLI to compute prime numbers"
```

**What happens**

1. Planner splits request into `[Doc]` and `[Code]` tasks.
2. Writer drafts Markdown docs; missing facts trigger DuckDuckGo research.
3. DevAgent (Openâ€‘Interpreter) edits files and runs tests; commits via Git.

All docs land in `/docs/*.md`.  Commits are timestamped autoâ€‘messages.

### 5.2Â Â Manager + LLMDev Pipeline

```powershell
python run_loop.py "Build a simple calculator that adds two numbers"
```

1. ManagerAgent creates a plan (CoT inside `<think>` tags).
2. LLMDevAgent writes & executes code; result inserted back into memory.
3. Manager reviews and outputs a userâ€‘friendly conclusion.

Runâ€loop demonstrates **shared `Memory`** trimming to stay under 1500Â tokens.

---

## 6Â Â Testing

* **Unit tests**: place `*_test.py` or `test_*.py` files; theyâ€™re autoâ€‘run by `pytest` inside the DevAgent fallback.
* **Manual**: inspect `.git` log to ensure only passing states are committed.
* **Timing**: each LLM call has a 300â€¯s timeout; if a model is slow on first load, subsequent calls are cached in RAM.

---

## 7Â Â Troubleshooting

| Symptom                                         | Fix                                                                       |
| ----------------------------------------------- | ------------------------------------------------------------------------- |
| `requests.exceptions.ConnectionError` to Ollama | Is Ollama running? `ollama serve`                                         |
| JSON key `'output'` missing                     | Upgrade Ollama â‰¥Â 0.1.23 **or** edit `agents.py` fallback to `'response'`. |
| GPU OOM                                         | Use quantised model (`:q4_K_M`) or switch to CPU (`OLLAMA_NUM_GPU=0`).    |
| Endless context growth                          | `Memory.trim()` thresholds can be lowered (e.g. `max_tokens=1000`).       |
| DuckDuckGo blocked                              | Set `DDG_OFFLINE=1` and stub `search_tool.ddg_search`.                    |

---

## 8Â Â Extending the System

* **Add more specialised agents** â€“ e.g. a TesterAgent that only writes `pytest` suites, then let DevAgent implement fixes.
* **Vector store memory** â€“ plug a lightweight Chroma DB; store full docs, keep only semantic summaries in prompt.
* **Async scheduler** â€“ wrap Ollama calls in an `asyncio.Lock` to allow background I/O while respecting singleâ€‘GPU inference.
* **GUI** â€“ build a simple Tk or Electron frontâ€‘end calling the same CLI scripts.

Pull requests welcome!  Enjoy your local multiâ€‘agent workstation ğŸš€
